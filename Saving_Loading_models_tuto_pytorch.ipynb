{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saving_Loading_models_tuto_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pioterr/Arch_linux/blob/master/Saving_Loading_models_tuto_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GkXH_8IgTMI"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!df -h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl2Ae-5jgyUO"
      },
      "source": [
        "# When it comes to saving and loading models,\n",
        "# there are three core functions to be familiar with:\n",
        "###############################################################################\n",
        "# torch.save: Saves a serialized object to disk.\n",
        "#     This function uses Python’s pickle utility for serialization.\n",
        "#     Models, tensors, and dictionaries of all kinds of\n",
        "#     objects can be saved using this function.\n",
        "###############################################################################\n",
        "# torch.load: Uses pickle’s unpickling facilities to deserialize\n",
        "#     pickled object files to memory. This function also facilitates the\n",
        "#     device to load the data into (see Saving & Loading Model Across Devices).\n",
        "###############################################################################\n",
        "# torch.nn.Module.load_state_dict: Loads a model’s parameter dictionary\n",
        "#     using a deserialized state_dict. For more information on state_dict.\n",
        "###############################################################################\n",
        "# Let’s take a look at the state_dict from the simple model\n",
        "# used in the Training a classifier tutorial\n",
        "\n",
        "# What is a ``state_dict``?\n",
        "# -------------------------\n",
        "#\n",
        "# In PyTorch, the learnable parameters (i.e. weights and biases) of an\n",
        "# ``torch.nn.Module`` model is contained in the model’s *parameters*\n",
        "# (accessed with ``model.parameters()``). A *state_dict* is simply a\n",
        "# Python dictionary object that maps each layer to its parameter tensor.\n",
        "# Note that only layers with learnable parameters (convolutional layers,\n",
        "# linear layers, etc.) have entries in the model’s *state_dict*. Optimizer\n",
        "# objects (``torch.optim``) also have a *state_dict*, which contains\n",
        "# information about the optimizer’s state, as well as the hyperparameters\n",
        "# used.\n",
        "#\n",
        "# Because *state_dict* objects are Python dictionaries, they can be easily\n",
        "# saved, updated, altered, and restored, adding a great deal of modularity\n",
        "# to PyTorch models and optimizers.\n",
        "#\n",
        "# Example:\n",
        "# ^^^^^^^^\n",
        "#\n",
        "# Let’s take a look at the *state_dict* from the simple model used in the\n",
        "# `Training a classifier tutorial.\n",
        "\n",
        "import torch\n",
        "\n",
        "# Define model\n",
        "\n",
        "\n",
        "\n",
        "class TheModelClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TheModelClass, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "model = TheModelClass()\n",
        "\n",
        "# Initialize optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_CUrveP5mzh"
      },
      "source": [
        "####### BOF ########\n",
        "\n",
        "# SAVING & LOADING MODEL FOR INFERENCE\n",
        "# Save/Load state_dict (Recommended)\n",
        "\n",
        "# Save:\n",
        "\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# Load:\n",
        "\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "# When saving a model for inference, it is only necessary to save the trained\n",
        "# model’s learned parameters. Saving the model’s state_dict with the torch.save()\n",
        "# function will give you the most flexibility for restoring the model later,\n",
        "# which is why it is the recommended method for saving models.\n",
        "\n",
        "# A common PyTorch convention is to save models\n",
        "# using either a .pt or .pth file extension.\n",
        "\n",
        "# Remember that you must call model.eval() to set dropout and batch\n",
        "# normalization layers to evaluation mode before running inference.\n",
        "# Failing to do this will yield inconsistent inference results.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS57jqWb6rWZ"
      },
      "source": [
        "####### BOF ########\n",
        "\n",
        "# Save/Load Entire Model\n",
        "\n",
        "# Save:\n",
        "\n",
        "torch.save(model, PATH)\n",
        "\n",
        "# Load:\n",
        "\n",
        "# Model class must be defined somewhere\n",
        "model = torch.load(PATH)\n",
        "model.eval()\n",
        "\n",
        "# This save/load process uses the most intuitive syntax and involves\n",
        "# the least amount of code. Saving a model in this way will save the\n",
        "# entire module using Python’s pickle module. The disadvantage of this\n",
        "# approach is that the serialized data is bound to the specific classes\n",
        "# and the exact directory structure used when the model is saved.\n",
        "# The reason for this is because pickle does not save the model class itself.\n",
        "# Rather, it saves a path to the file containing the class, which is used during\n",
        "# load time. Because of this, your code can break in various ways when used in\n",
        "# other projects or after refactors.\n",
        "\n",
        "# A common PyTorch convention is to save models using either a .pt or .pth\n",
        "# file extension.\n",
        "\n",
        "# Remember that you must call model.eval() to set dropout and\n",
        "# batch normalization layers to evaluation mode before running inference.\n",
        "# Failing to do this will yield inconsistent inference results."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKotXu8R7DYx"
      },
      "source": [
        "####### OUI ########\n",
        "\n",
        "# SAVING & LOADING A GENERAL CHECKPOINT FOR INFERENCE AND/OR RESUMING TRAINING\n",
        "# Save:\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            ...\n",
        "            }, PATH)\n",
        "# Load:\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# - or -\n",
        "model.train()\n",
        "\n",
        "# When saving a general checkpoint, to be used for either inference or resuming\n",
        "# training, you must save more than just the model’s state_dict.\n",
        "# It is important to also save the optimizer’s state_dict, as this contains\n",
        "# buffers and parameters that are updated as the model trains. Other items that\n",
        "# you may want to save are the epoch you left off on, the latest recorded\n",
        "# training loss, external torch.nn.Embedding layers, etc.\n",
        "\n",
        "# To save multiple components, organize them in a dictionary and use torch.save()\n",
        "# to serialize the dictionary. A common PyTorch convention is to save\n",
        "# these checkpoints using the .tar file extension.\n",
        "\n",
        "# To load the items, first initialize the model and optimizer, then \n",
        "# oad the dictionary locally using torch.load(). From here, you can easily\n",
        "# access the saved items by simply querying the dictionary as you would expect.\n",
        "\n",
        "# Remember that you must call model.eval() to set dropout and batch normalization\n",
        "# layers to evaluation mode before running inference. Failing to do this will\n",
        "# yield inconsistent inference results.\n",
        "# If you wish to resuming training,\n",
        "# call model.train() to ensure these layers are in training mode."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y_KOItm7DqJ"
      },
      "source": [
        "# SAVING MULTIPLE MODELS IN ONE FILE\n",
        "# Save:\n",
        "torch.save({\n",
        "            'modelA_state_dict': modelA.state_dict(),\n",
        "            'modelB_state_dict': modelB.state_dict(),\n",
        "            'optimizerA_state_dict': optimizerA.state_dict(),\n",
        "            'optimizerB_state_dict': optimizerB.state_dict(),\n",
        "            ...\n",
        "            }, PATH)\n",
        "# Load:\n",
        "modelA = TheModelAClass(*args, **kwargs)\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
        "optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "modelA.eval()\n",
        "modelB.eval()\n",
        "# - or -\n",
        "modelA.train()\n",
        "modelB.train()\n",
        "\n",
        "# When saving a model comprised of multiple torch.nn.Modules, such as a GAN,\n",
        "# a sequence-to-sequence model, or an ensemble of models, you follow the same\n",
        "# approach as when you are saving a general checkpoint. In other words, save a\n",
        "# dictionary of each model’s state_dict and corresponding optimizer.\n",
        "# As mentioned before, you can save any other items that may aid you in\n",
        "# resuming training by simply appending them to the dictionary.\n",
        "\n",
        "# A common PyTorch convention is to save these checkpoints using the .tar\n",
        "# file extension.\n",
        "\n",
        "# To load the models, first initialize the models and optimizers,\n",
        "# then load the dictionary locally using torch.load(). From here, you can easily\n",
        "# access the saved items by simply querying the dictionary as you would expect.\n",
        "\n",
        "# Remember that you must call model.eval() to set dropout and batch normalization\n",
        "# layers to evaluation mode before running inference. Failing to do this will\n",
        "# yield inconsistent inference results. If you wish to resuming training, call\n",
        "# model.train() to set these layers to training mode."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_4Aw3PW7DsF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}